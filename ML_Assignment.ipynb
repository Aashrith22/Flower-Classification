{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ac9ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (144, 128, 128, 3)\n",
      "Validation set shape: (48, 128, 128, 3)\n",
      "Test set shape: (48, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "# Loading and Preprocessing Images\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to load and preprocess the images\n",
    "\n",
    "def load_images(directory, image_size):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(directory):\n",
    "        image_path = os.path.join(directory, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, image_size)  # Resize images to a uniform size\n",
    "        image = image.astype(np.float32) / 255.0  # Normalize pixel values to [0, 1]\n",
    "        images.append(image)\n",
    "        # Assuming filename format is \"class_filename.jpg\"\n",
    "        class_name = filename.split('_')[0]\n",
    "        labels.append(class_name)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Define directory\n",
    "data_directory = r\"C:\\Users\\Dell\\Documents\\Dataset\"\n",
    "\n",
    "# Define image size for resizing\n",
    "image_size = (128, 128)\n",
    "\n",
    "# Load and preprocess images\n",
    "\n",
    "images, labels = load_images(data_directory, image_size)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Print shapes of the datasets\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb161964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotation of Images using Bounding Boxes\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Function to annotate images with bounding boxes\n",
    "\n",
    "def annotate_with_bounding_boxes(image_paths, annotations_file, batch_size=10):\n",
    "    annotations = []\n",
    "    num_images = len(image_paths)\n",
    "\n",
    "    # Creating a named window for displaying images\n",
    "    cv2.namedWindow(\"Annotate with Bounding Boxes\")\n",
    "\n",
    "    for i in range(0, num_images, batch_size):\n",
    "        batch_image_paths = image_paths[i:i+batch_size]\n",
    "\n",
    "        for image_path in batch_image_paths:\n",
    "            # Loading the Image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "\n",
    "            # Displaying the image and annotating with bounding box\n",
    "\n",
    "            bounding_box = cv2.selectROI(\"Annotate with Bounding Boxes\", image, fromCenter=False, showCrosshair=True)\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "            x, y, w, h = bounding_box\n",
    "            x_min, y_min, x_max, y_max = x, y, x + w, y + h\n",
    "\n",
    "            # Creating an annotation dictionary\n",
    "\n",
    "            annotation = {\n",
    "                \"image_path\": image_path,\n",
    "                \"bounding_box\": [int(x_min), int(y_min), int(x_max), int(y_max)]\n",
    "            }\n",
    "            annotations.append(annotation)\n",
    "\n",
    "    # Saving all annotations to a JSON file\n",
    "    with open(annotations_file, \"w\") as f:\n",
    "        json.dump(annotations, f, indent=4)\n",
    "\n",
    "    # Destroy or Killing the named window created above\n",
    "    if cv2.getWindowProperty(\"Annotate with Bounding Boxes\", cv2.WND_PROP_VISIBLE) >= 1:\n",
    "        cv2.destroyWindow(\"Annotate with Bounding Boxes\")\n",
    "\n",
    "# Defining the directory containing the images\n",
    "data_directory = r\"C:\\Users\\Dell\\Documents\\Dataset\"\n",
    "\n",
    "# Getting paths of all image files in the directory\n",
    "image_paths = [os.path.join(data_directory, filename) for filename in os.listdir(data_directory) if filename.endswith(\".jpg\")]\n",
    "\n",
    "# Setting path for annotations file\n",
    "annotations_file = \"flower_annotations.json\"\n",
    "\n",
    "# Annotating images with bounding boxes and save annotations to file\n",
    "annotate_with_bounding_boxes(image_paths, annotations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31f1f7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "5/5 [==============================] - 3s 332ms/step - loss: 1.1709 - accuracy: 0.4236 - val_loss: 1.0941 - val_accuracy: 0.3125\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 251ms/step - loss: 0.9781 - accuracy: 0.4583 - val_loss: 0.8710 - val_accuracy: 0.5208\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 252ms/step - loss: 0.8070 - accuracy: 0.6319 - val_loss: 0.7067 - val_accuracy: 0.6667\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 1s 259ms/step - loss: 0.6336 - accuracy: 0.7431 - val_loss: 0.5882 - val_accuracy: 0.7083\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 265ms/step - loss: 0.5297 - accuracy: 0.7986 - val_loss: 0.4909 - val_accuracy: 0.7917\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 258ms/step - loss: 0.4197 - accuracy: 0.8403 - val_loss: 0.4912 - val_accuracy: 0.7708\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 1s 255ms/step - loss: 0.3212 - accuracy: 0.8681 - val_loss: 0.6977 - val_accuracy: 0.6250\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 1s 257ms/step - loss: 0.2717 - accuracy: 0.8819 - val_loss: 0.4481 - val_accuracy: 0.7917\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 1s 263ms/step - loss: 0.1923 - accuracy: 0.9722 - val_loss: 0.4976 - val_accuracy: 0.7917\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 1s 262ms/step - loss: 0.1039 - accuracy: 0.9722 - val_loss: 0.5426 - val_accuracy: 0.8333\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5956 - accuracy: 0.8125\n",
      "Test Loss: 0.5955998301506042\n",
      "Test Accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Function to load and preprocess the images\n",
    "\n",
    "def load_images(directory, image_size):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_names = sorted(os.listdir(directory))\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        for filename in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, filename)\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.resize(image, image_size)  # Resizing images to a uniform size\n",
    "            image = image.astype(np.float32) / 255.0  # Normalizing pixel values to [0, 1]\n",
    "            images.append(image)\n",
    "            labels.append(class_names.index(class_name))\n",
    "    \n",
    "    return np.array(images), np.array(labels), num_classes\n",
    "\n",
    "# Directory containing the images\n",
    "data_directory = r\"C:\\Users\\Dell\\Documents\\Dataset\"\n",
    "\n",
    "# Defining image size for resizing\n",
    "image_size = (128, 128)  # Adjust as needed\n",
    "\n",
    "# Load and preprocess images\n",
    "images, labels, num_classes = load_images(data_directory, image_size)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Define CNN model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(image_size[0], image_size[1], 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Saving the model\n",
    "model.save(\"flower_classification_model.h5\")\n",
    "\n",
    "# Saving the model history\n",
    "with open(\"model_history.json\", \"w\") as f:\n",
    "    json.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1344cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "1/1 [==============================] - 0s 441ms/step\n",
      "Predicted Class: Class1\n"
     ]
    }
   ],
   "source": [
    "# Testing the Model on Data from different Directory\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Loading the trained model which is created above\n",
    "model = load_model(\"flower_classification_model.h5\")\n",
    "\n",
    "# Load and preprocess the input image\n",
    "def preprocess_image(image_path, target_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, target_size)\n",
    "    image = image.astype(np.float32) / 255.0  # Normalize pixel values\n",
    "    return np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "\n",
    "# Defining the target image size\n",
    "target_size = (128, 128)\n",
    "\n",
    "# Path to the input image\n",
    "input_image_path = r\"C:/Users/Dell/Downloads/image_0001.jpg\"\n",
    "\n",
    "# Preprocessing the input image\n",
    "input_image = preprocess_image(input_image_path, target_size)\n",
    "\n",
    "# Performing inference to get class probabilities\n",
    "class_probabilities = model.predict(input_image)\n",
    "\n",
    "# Predicted class index (class with highest probability)\n",
    "predicted_class_index = np.argmax(class_probabilities)\n",
    "\n",
    "# Class labels\n",
    "class_labels = ['Class1', 'Class2', 'Class3']\n",
    "\n",
    "# Output\n",
    "predicted_class_label = class_labels[predicted_class_index]\n",
    "print(\"Predicted Class:\", predicted_class_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
